{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f745db64-fc8c-4c26-a290-e92b191d649e",
   "metadata": {},
   "source": [
    "## Amazon Dataset Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7ec224-2498-423e-8d62-9fa1dd9ca81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\filip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Reading file: 51224it [00:00, 294461.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 reviews (balanced dataset).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Stuning even for the non-gamer This sound trac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything. I'm read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Amazing! This soundtrack is my favorite music ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Excellent Soundtrack I truly like this soundtr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      1  Stuning even for the non-gamer This sound trac...\n",
       "1      1  The best soundtrack ever to anything. I'm read...\n",
       "2      1  Amazing! This soundtrack is my favorite music ...\n",
       "3      1  Excellent Soundtrack I truly like this soundtr...\n",
       "4      1  Remember, Pull Your Jaw Off The Floor After He..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Run only once if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Setup\n",
    "file_path = \"train.ft.txt\"\n",
    "max_per_class = 25000\n",
    "min_text_len = 3\n",
    "\n",
    "# Initialize\n",
    "data = []\n",
    "label_counts = {0: 0, 1: 0}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Step 1: Read and filter lines\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in tqdm(f, desc=\"Reading file\"):\n",
    "        match = re.match(r\"__label__(\\d) (.+)\", line)\n",
    "        if not match:\n",
    "            continue\n",
    "\n",
    "        label_raw, text = int(match.group(1)), match.group(2).strip()\n",
    "        if label_raw not in [1, 2]:\n",
    "            continue\n",
    "\n",
    "        label = 0 if label_raw == 1 else 1\n",
    "        if label_counts[label] >= max_per_class:\n",
    "            continue\n",
    "\n",
    "        # Split title + rest of review\n",
    "        parts = text.split(\":\", 1)\n",
    "        if len(parts) == 2:\n",
    "            title, body = parts\n",
    "            full_text = (title.strip() + \" \" + body.strip()).strip()\n",
    "        else:\n",
    "            full_text = text.strip()\n",
    "\n",
    "        # Filter short/empty reviews\n",
    "        if len(full_text) < min_text_len:\n",
    "            continue\n",
    "\n",
    "        data.append((label, full_text))\n",
    "        label_counts[label] += 1\n",
    "\n",
    "        if label_counts[0] >= max_per_class and label_counts[1] >= max_per_class:\n",
    "            break\n",
    "\n",
    "# Step 2: Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"label\", \"review\"])\n",
    "print(f\"Loaded {len(df)} reviews (balanced dataset).\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d9e7e2-0318-4ea5-9380-944efca354d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  I really liked this Summerslam due to the look...      1\n",
       "1  Not many television shows appeal to quite as m...      1\n",
       "2  The film quickly gets to a major chase scene w...      0\n",
       "3  Jane Austen would definitely approve of this o...      1\n",
       "4  Expectations were somewhat high for me when I ...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imdb = pd.read_csv(\"IMDB Dataset.csv\")  \n",
    "df_imdb = df_imdb.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "# Encode labels: positive -> 1, negative -> 0\n",
    "df_imdb['label'] = df_imdb['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df_imdb = df_imdb.drop(columns=['sentiment'])\n",
    "print(\"Total samples:\", len(df_imdb))\n",
    "df_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db9dc0a-db8b-42ee-878f-2e526147e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_imdb], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8eb9ec8-415a-4b4b-9b1f-23e8ae11445c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>It was funny because the whole thing was so un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I've read innumerable reviews talking about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great Foot Cream Recently purchased a jar of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>I watched this movie a couple months ago when ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>I just re-watched a few episodes of this serie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             review\n",
       "0      0  It was funny because the whole thing was so un...\n",
       "1      1  I've read innumerable reviews talking about th...\n",
       "2      1  Great Foot Cream Recently purchased a jar of t...\n",
       "3      0  I watched this movie a couple months ago when ...\n",
       "4      1  I just re-watched a few episodes of this serie..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
    "\n",
    "print(\"Total samples:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a79f23d-d7d4-4b85-9284-1f8579a4eaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text: 100%|██████████| 100000/100000 [01:21<00:00, 1219.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'review', 'cleaned_review'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning with progress bar\n",
    "tqdm.pandas(desc=\"Cleaning text\")\n",
    "df[\"cleaned_review\"] = df[\"review\"].progress_apply(clean_text)\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a8af41-ddf8-4d53-b915-1fdf0442470f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'cleaned_review'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=['review'])\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed9c84e6-9179-4ac0-9fa2-a366b058e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def bag_of_words(textos):\n",
    "    vectorizer = CountVectorizer()\n",
    "    actual = []\n",
    "    for i in range(len(textos)):\n",
    "        actual.append(textos[i])\n",
    "\n",
    "    X = vectorizer.fit_transform(actual)\n",
    "    diccionario = vectorizer.get_feature_names_out()\n",
    "    bow = X.toarray()\n",
    "    return diccionario, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5e5260-7331-4bb0-be72-541401ef3243",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 147. GiB for an array with shape (100000, 197787) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m diccionario, X \u001b[38;5;241m=\u001b[39m \u001b[43mbag_of_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mbag_of_words\u001b[1;34m(textos)\u001b[0m\n\u001b[0;32m      9\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(actual)\n\u001b[0;32m     10\u001b[0m diccionario \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m---> 11\u001b[0m bow \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diccionario, bow\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 147. GiB for an array with shape (100000, 197787) and data type int64"
     ]
    }
   ],
   "source": [
    "diccionario, X = bag_of_words(df['cleaned_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f3e34a7-e09b-4bd3-b9bd-9972025096ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizing with TF-IDF:   0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   1%|          | 558/100000 [00:00<00:18, 5347.48it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   1%|          | 1096/100000 [00:00<00:18, 5301.07it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   2%|▏         | 1627/100000 [00:00<00:20, 4896.24it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   2%|▏         | 2161/100000 [00:00<00:20, 4871.20it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   3%|▎         | 2675/100000 [00:00<00:20, 4787.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   3%|▎         | 3181/100000 [00:00<00:20, 4714.19it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   4%|▎         | 3699/100000 [00:00<00:20, 4710.92it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   4%|▍         | 4171/100000 [00:00<00:20, 4661.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   5%|▍         | 4638/100000 [00:00<00:20, 4643.85it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   5%|▌         | 5103/100000 [00:01<00:22, 4205.59it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   6%|▌         | 5572/100000 [00:01<00:21, 4337.33it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   6%|▌         | 6012/100000 [00:01<00:21, 4354.72it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   6%|▋         | 6452/100000 [00:01<00:21, 4363.43it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   7%|▋         | 6976/100000 [00:01<00:21, 4418.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   7%|▋         | 7420/100000 [00:01<00:21, 4383.81it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   8%|▊         | 7860/100000 [00:01<00:21, 4358.31it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   8%|▊         | 8297/100000 [00:01<00:22, 4051.04it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   9%|▊         | 8707/100000 [00:01<00:23, 3927.38it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   9%|▉         | 9103/100000 [00:02<00:23, 3851.34it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:   9%|▉         | 9490/100000 [00:02<00:23, 3797.07it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  10%|▉         | 9871/100000 [00:02<00:27, 3225.53it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  10%|█         | 10261/100000 [00:02<00:26, 3394.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  11%|█         | 10714/100000 [00:02<00:24, 3640.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  11%|█         | 11236/100000 [00:02<00:22, 3960.62it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  12%|█▏        | 11678/100000 [00:02<00:22, 3968.96it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  12%|█▏        | 12158/100000 [00:02<00:21, 4086.00it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  13%|█▎        | 12661/100000 [00:02<00:20, 4229.40it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  13%|█▎        | 13088/100000 [00:03<00:21, 4112.27it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  14%|█▎        | 13560/100000 [00:03<00:20, 4206.77it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  14%|█▍        | 13983/100000 [00:03<00:20, 4184.05it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  14%|█▍        | 14464/100000 [00:03<00:20, 4270.33it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  15%|█▍        | 14903/100000 [00:03<00:20, 4181.98it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  15%|█▌        | 15356/100000 [00:03<00:19, 4265.13it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  16%|█▌        | 15784/100000 [00:03<00:19, 4218.20it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  16%|█▋        | 16259/100000 [00:03<00:19, 4230.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  17%|█▋        | 16683/100000 [00:03<00:20, 4037.79it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  17%|█▋        | 17089/100000 [00:04<00:21, 3805.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  18%|█▊        | 17518/100000 [00:04<00:21, 3827.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  18%|█▊        | 17939/100000 [00:04<00:20, 3931.30it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  18%|█▊        | 18340/100000 [00:04<00:21, 3883.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  19%|█▊        | 18730/100000 [00:04<00:21, 3792.91it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  19%|█▉        | 19116/100000 [00:04<00:21, 3765.79it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  20%|█▉        | 19551/100000 [00:04<00:20, 3897.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  20%|█▉        | 19942/100000 [00:04<00:30, 2631.14it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  20%|██        | 20380/100000 [00:05<00:27, 2940.98it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  21%|██        | 20721/100000 [00:05<00:26, 2978.53it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  21%|██        | 21099/100000 [00:05<00:25, 3147.85it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  22%|██▏       | 21536/100000 [00:05<00:22, 3461.69it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  22%|██▏       | 21990/100000 [00:05<00:20, 3749.15it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  22%|██▏       | 22409/100000 [00:05<00:20, 3868.90it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  23%|██▎       | 22854/100000 [00:05<00:19, 4032.85it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  23%|██▎       | 23269/100000 [00:05<00:19, 3897.57it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  24%|██▎       | 23694/100000 [00:05<00:19, 3880.83it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  24%|██▍       | 24169/100000 [00:06<00:19, 3988.92it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  25%|██▍       | 24629/100000 [00:06<00:18, 4046.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  25%|██▌       | 25153/100000 [00:06<00:17, 4249.52it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  26%|██▌       | 25580/100000 [00:06<00:17, 4180.23it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  26%|██▌       | 26050/100000 [00:06<00:17, 4149.13it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  26%|██▋       | 26466/100000 [00:06<00:18, 4000.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  27%|██▋       | 26889/100000 [00:06<00:18, 3979.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  27%|██▋       | 27313/100000 [00:06<00:17, 4049.21it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  28%|██▊       | 27728/100000 [00:06<00:17, 4077.53it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  28%|██▊       | 28202/100000 [00:07<00:17, 4082.17it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  29%|██▊       | 28611/100000 [00:07<00:18, 3951.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  29%|██▉       | 29028/100000 [00:07<00:18, 3892.63it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  29%|██▉       | 29422/100000 [00:07<00:18, 3779.46it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  30%|██▉       | 29925/100000 [00:07<00:17, 4007.13it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  30%|███       | 30327/100000 [00:07<00:17, 3984.43it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  31%|███       | 30780/100000 [00:07<00:16, 4102.49it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  31%|███       | 31191/100000 [00:07<00:17, 4002.86it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  32%|███▏      | 31607/100000 [00:07<00:17, 4009.58it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  32%|███▏      | 32046/100000 [00:08<00:16, 4053.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  32%|███▏      | 32452/100000 [00:08<00:17, 3928.82it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  33%|███▎      | 32848/100000 [00:08<00:17, 3883.50it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  33%|███▎      | 33237/100000 [00:08<00:18, 3700.52it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  34%|███▎      | 33641/100000 [00:08<00:18, 3681.31it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  34%|███▍      | 34057/100000 [00:08<00:17, 3704.98it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  34%|███▍      | 34436/100000 [00:08<00:18, 3617.91it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  35%|███▍      | 34859/100000 [00:08<00:17, 3673.05it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  35%|███▌      | 35291/100000 [00:08<00:17, 3734.29it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  36%|███▌      | 35708/100000 [00:09<00:17, 3741.11it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  36%|███▌      | 36088/100000 [00:09<00:17, 3646.31it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  36%|███▋      | 36453/100000 [00:09<00:17, 3538.96it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  37%|███▋      | 36817/100000 [00:09<00:18, 3464.01it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  37%|███▋      | 37222/100000 [00:09<00:17, 3523.69it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  38%|███▊      | 37581/100000 [00:09<00:18, 3439.17it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  38%|███▊      | 38012/100000 [00:09<00:17, 3573.22it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  38%|███▊      | 38447/100000 [00:09<00:16, 3686.01it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  39%|███▉      | 38866/100000 [00:09<00:16, 3714.89it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  39%|███▉      | 39292/100000 [00:09<00:15, 3862.55it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  40%|███▉      | 39711/100000 [00:10<00:15, 3908.82it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  40%|████      | 40121/100000 [00:10<00:15, 3919.97it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  41%|████      | 40535/100000 [00:10<00:15, 3927.54it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  41%|████      | 40929/100000 [00:10<00:29, 1972.41it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  41%|████▏     | 41301/100000 [00:10<00:25, 2259.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  42%|████▏     | 41739/100000 [00:10<00:21, 2661.28it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  42%|████▏     | 42091/100000 [00:11<00:20, 2778.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  43%|████▎     | 42558/100000 [00:11<00:18, 3139.92it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  43%|████▎     | 43027/100000 [00:11<00:16, 3474.63it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  43%|████▎     | 43450/100000 [00:11<00:15, 3657.04it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  44%|████▍     | 43848/100000 [00:11<00:15, 3634.62it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  44%|████▍     | 44268/100000 [00:11<00:15, 3678.63it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  45%|████▍     | 44691/100000 [00:11<00:14, 3761.46it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  45%|████▌     | 45120/100000 [00:11<00:14, 3771.67it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  46%|████▌     | 45506/100000 [00:11<00:14, 3704.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  46%|████▌     | 45882/100000 [00:12<00:14, 3718.32it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  46%|████▋     | 46280/100000 [00:12<00:14, 3680.46it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  47%|████▋     | 46775/100000 [00:12<00:13, 3908.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  47%|████▋     | 47168/100000 [00:12<00:14, 3761.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  48%|████▊     | 47635/100000 [00:12<00:13, 3937.38it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  48%|████▊     | 48132/100000 [00:12<00:12, 4102.38it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  49%|████▊     | 48621/100000 [00:12<00:12, 4196.81it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  49%|████▉     | 49042/100000 [00:12<00:12, 4181.65it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  49%|████▉     | 49461/100000 [00:12<00:12, 3956.31it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  50%|████▉     | 49861/100000 [00:13<00:12, 3909.72it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  50%|█████     | 50292/100000 [00:13<00:12, 4021.81it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  51%|█████     | 50696/100000 [00:13<00:12, 4025.64it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  51%|█████     | 51100/100000 [00:13<00:12, 4028.93it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  52%|█████▏    | 51504/100000 [00:13<00:12, 4002.94it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  52%|█████▏    | 51921/100000 [00:13<00:11, 4051.09it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  52%|█████▏    | 52327/100000 [00:13<00:11, 3995.34it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  53%|█████▎    | 52729/100000 [00:13<00:11, 3966.69it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  53%|█████▎    | 53126/100000 [00:13<00:12, 3749.61it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  54%|█████▎    | 53504/100000 [00:13<00:12, 3649.86it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  54%|█████▍    | 53883/100000 [00:14<00:12, 3596.48it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  54%|█████▍    | 54305/100000 [00:14<00:12, 3631.58it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  55%|█████▍    | 54799/100000 [00:14<00:11, 3873.07it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  55%|█████▌    | 55240/100000 [00:14<00:11, 3980.66it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  56%|█████▌    | 55639/100000 [00:14<00:11, 3962.52it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  56%|█████▌    | 56129/100000 [00:14<00:10, 4101.81it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  57%|█████▋    | 56594/100000 [00:14<00:10, 4148.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  57%|█████▋    | 57034/100000 [00:14<00:10, 4218.93it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  57%|█████▋    | 57457/100000 [00:14<00:10, 4221.07it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  58%|█████▊    | 57880/100000 [00:15<00:09, 4219.61it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  58%|█████▊    | 58303/100000 [00:15<00:10, 4059.23it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  59%|█████▉    | 58761/100000 [00:15<00:10, 4083.72it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  59%|█████▉    | 59171/100000 [00:15<00:10, 3968.77it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  60%|█████▉    | 59599/100000 [00:15<00:10, 3970.00it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  60%|█████▉    | 59997/100000 [00:15<00:10, 3663.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  60%|██████    | 60425/100000 [00:15<00:10, 3727.63it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  61%|██████    | 60878/100000 [00:15<00:10, 3835.15it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  61%|██████▏   | 61275/100000 [00:15<00:10, 3871.27it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  62%|██████▏   | 61750/100000 [00:16<00:09, 4050.72it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  62%|██████▏   | 62186/100000 [00:16<00:09, 4118.46it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  63%|██████▎   | 62616/100000 [00:16<00:08, 4167.28it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  63%|██████▎   | 63034/100000 [00:16<00:09, 4026.11it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  64%|██████▎   | 63500/100000 [00:16<00:08, 4091.14it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  64%|██████▍   | 63940/100000 [00:16<00:08, 4064.19it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  64%|██████▍   | 64417/100000 [00:16<00:08, 4149.01it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  65%|██████▍   | 64861/100000 [00:16<00:08, 4115.76it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  65%|██████▌   | 65308/100000 [00:16<00:08, 4102.82it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  66%|██████▌   | 65765/100000 [00:16<00:08, 4117.28it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  66%|██████▌   | 66182/100000 [00:17<00:08, 4022.22it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  67%|██████▋   | 66636/100000 [00:17<00:08, 4056.42it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  67%|██████▋   | 67042/100000 [00:17<00:08, 3932.79it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  67%|██████▋   | 67436/100000 [00:17<00:08, 3682.74it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  68%|██████▊   | 67903/100000 [00:17<00:08, 3849.11it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  68%|██████▊   | 68351/100000 [00:17<00:08, 3917.40it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  69%|██████▉   | 68794/100000 [00:17<00:07, 3952.10it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  69%|██████▉   | 69237/100000 [00:17<00:07, 3976.92it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  70%|██████▉   | 69637/100000 [00:18<00:07, 3877.71it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  70%|███████   | 70054/100000 [00:18<00:07, 3852.62it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  71%|███████   | 70505/100000 [00:18<00:07, 3928.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  71%|███████   | 70936/100000 [00:18<00:07, 3760.62it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  71%|███████▏  | 71358/100000 [00:18<00:07, 3778.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  72%|███████▏  | 71783/100000 [00:18<00:07, 3804.54it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  72%|███████▏  | 72215/100000 [00:18<00:07, 3842.63it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  73%|███████▎  | 72655/100000 [00:18<00:07, 3890.81it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  73%|███████▎  | 73066/100000 [00:18<00:07, 3847.53it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  74%|███████▎  | 73506/100000 [00:19<00:06, 3895.52it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  74%|███████▍  | 73941/100000 [00:19<00:06, 3914.72it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  74%|███████▍  | 74379/100000 [00:19<00:06, 3942.79it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  75%|███████▍  | 74819/100000 [00:19<00:06, 3996.41it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  75%|███████▌  | 75219/100000 [00:19<00:06, 3993.57it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  76%|███████▌  | 75619/100000 [00:19<00:06, 3912.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  76%|███████▌  | 76011/100000 [00:19<00:06, 3502.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  76%|███████▋  | 76429/100000 [00:19<00:06, 3588.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  77%|███████▋  | 76857/100000 [00:19<00:06, 3676.98it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  77%|███████▋  | 77305/100000 [00:20<00:05, 3795.48it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  78%|███████▊  | 77733/100000 [00:20<00:05, 3825.87it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  78%|███████▊  | 78150/100000 [00:20<00:05, 3817.75it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  79%|███████▊  | 78574/100000 [00:20<00:05, 3895.90it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  79%|███████▉  | 78966/100000 [00:20<00:05, 3901.08it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  79%|███████▉  | 79358/100000 [00:20<00:05, 3903.30it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  80%|███████▉  | 79750/100000 [00:20<00:05, 3906.60it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  80%|████████  | 80142/100000 [00:20<00:05, 3764.03it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  81%|████████  | 80523/100000 [00:20<00:05, 3673.11it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  81%|████████  | 80957/100000 [00:20<00:05, 3758.74it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  81%|████████▏ | 81389/100000 [00:21<00:04, 3812.19it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  82%|████████▏ | 81817/100000 [00:21<00:04, 3838.18it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  82%|████████▏ | 82244/100000 [00:21<00:04, 3852.24it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  83%|████████▎ | 82674/100000 [00:21<00:04, 3871.92it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  83%|████████▎ | 83104/100000 [00:21<00:04, 3884.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  84%|████████▎ | 83553/100000 [00:21<00:04, 3946.78it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  84%|████████▍ | 84008/100000 [00:21<00:03, 4005.80it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  84%|████████▍ | 84409/100000 [00:22<00:12, 1292.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  85%|████████▍ | 84825/100000 [00:22<00:09, 1603.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  85%|████████▌ | 85145/100000 [00:22<00:08, 1718.32it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  86%|████████▌ | 85504/100000 [00:22<00:07, 2013.30it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  86%|████████▌ | 85914/100000 [00:23<00:05, 2396.03it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  86%|████████▋ | 86336/100000 [00:23<00:04, 2774.13it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  87%|████████▋ | 86746/100000 [00:23<00:04, 3076.16it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  87%|████████▋ | 87139/100000 [00:23<00:03, 3284.48it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  88%|████████▊ | 87549/100000 [00:23<00:03, 3495.39it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  88%|████████▊ | 87940/100000 [00:23<00:03, 3591.36it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  88%|████████▊ | 88330/100000 [00:23<00:03, 3578.84it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  89%|████████▉ | 88764/100000 [00:23<00:03, 3689.38it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  89%|████████▉ | 89205/100000 [00:23<00:02, 3787.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  90%|████████▉ | 89628/100000 [00:23<00:02, 3807.56it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  90%|█████████ | 90028/100000 [00:24<00:02, 3758.78it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  90%|█████████ | 90409/100000 [00:24<00:02, 3672.68it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  91%|█████████ | 90824/100000 [00:24<00:02, 3705.28it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  91%|█████████ | 91232/100000 [00:24<00:02, 3708.89it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  92%|█████████▏| 91659/100000 [00:24<00:02, 3763.37it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  92%|█████████▏| 92067/100000 [00:24<00:02, 3749.59it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  92%|█████████▏| 92443/100000 [00:24<00:02, 3730.17it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  93%|█████████▎| 92817/100000 [00:24<00:01, 3709.78it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  93%|█████████▎| 93190/100000 [00:24<00:01, 3614.04it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  94%|█████████▎| 93581/100000 [00:25<00:01, 3598.27it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  94%|█████████▍| 93972/100000 [00:25<00:01, 3587.39it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  94%|█████████▍| 94399/100000 [00:25<00:01, 3679.06it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  95%|█████████▍| 94844/100000 [00:25<00:01, 3792.14it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  95%|█████████▌| 95318/100000 [00:25<00:01, 3950.75it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  96%|█████████▌| 95750/100000 [00:25<00:01, 3946.07it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  96%|█████████▌| 96145/100000 [00:25<00:01, 3405.61it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  97%|█████████▋| 96561/100000 [00:25<00:00, 3510.13it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  97%|█████████▋| 97002/100000 [00:25<00:00, 3653.39it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  97%|█████████▋| 97437/100000 [00:26<00:00, 3742.19it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  98%|█████████▊| 97877/100000 [00:26<00:00, 3819.25it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  98%|█████████▊| 98318/100000 [00:26<00:00, 3877.28it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  99%|█████████▉| 98775/100000 [00:26<00:00, 3962.00it/s]\u001b[A\n",
      "Vectorizing with TF-IDF:  99%|█████████▉| 99263/100000 [00:26<00:00, 4105.65it/s]\u001b[A\n",
      "Vectorizing with TF-IDF: 100%|██████████| 100000/100000 [00:26<00:00, 3744.01it/s][A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (100000, 1419267)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),   # Unigram + bigram + trigram\n",
    "    analyzer='word',\n",
    "    strip_accents='unicode',\n",
    "    min_df=2,           # Ignore terms appearing in less than 3 reviews\n",
    "    max_df=0.9          # Ignore terms appearing in more than 80% of reviews\n",
    ")\n",
    "\n",
    "# Apply TF-IDF\n",
    "X = vectorizer.fit_transform(tqdm(df[\"cleaned_review\"], desc=\"Vectorizing with TF-IDF\"))\n",
    "print(f\"TF-IDF matrix shape: {X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bbf5ca-aa19-4382-8f19-6851de7d2c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as 'cleaned_reviews_amazon.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"cleaned_reviews_amazon.csv\", index=False)\n",
    "print(\"File saved as 'cleaned_reviews_amazon.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4269519-815b-4a23-b62f-f53319e63e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70000\n",
      "Validation size: 15000\n",
      "Test size: 15000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, df[\"label\"], test_size=0.30, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]}\")\n",
    "print(f\"Validation size: {X_val.shape[0]}\")\n",
    "print(f\"Test size: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7012dc13-f2f2-478b-9dbd-7060e263a96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88      7500\n",
      "           1       0.88      0.89      0.88      7500\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n",
      "\n",
      "Linear SVM (SGD):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.87      0.87      7500\n",
      "           1       0.87      0.88      0.88      7500\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_val)\n",
    "print(\"\\nLogistic Regression:\")\n",
    "print(classification_report(y_val, y_pred_lr))\n",
    "\n",
    "# Linear SVM via SGDClassifier\n",
    "svm = SGDClassifier(loss=\"hinge\", max_iter=1000, random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred_svm = svm.predict(X_val)\n",
    "print(\"\\nLinear SVM (SGD):\")\n",
    "print(classification_report(y_val, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589b252d-2a04-4763-813b-2329519cb0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3, estimator=SGDClassifier(random_state=42), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01],\n",
       "                                        &#x27;eta0&#x27;: [0.01, 0.1, 0.5, 1],\n",
       "                                        &#x27;learning_rate&#x27;: [&#x27;optimal&#x27;,\n",
       "                                                          &#x27;invscaling&#x27;,\n",
       "                                                          &#x27;adaptive&#x27;],\n",
       "                                        &#x27;loss&#x27;: [&#x27;hinge&#x27;, &#x27;log_loss&#x27;,\n",
       "                                                 &#x27;modified_huber&#x27;],\n",
       "                                        &#x27;max_iter&#x27;: [2000, 2500, 3000, 4000,\n",
       "                                                     5000],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l2&#x27;, &#x27;l1&#x27;, &#x27;elasticnet&#x27;]},\n",
       "                   random_state=42,\n",
       "                   scoring=make_scorer(f1_score, response_method=&#x27;predict&#x27;, average=macro),\n",
       "                   verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(cv=3, estimator=SGDClassifier(random_state=42), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={&#x27;alpha&#x27;: [1e-05, 0.0001, 0.001, 0.01],\n",
       "                                        &#x27;eta0&#x27;: [0.01, 0.1, 0.5, 1],\n",
       "                                        &#x27;learning_rate&#x27;: [&#x27;optimal&#x27;,\n",
       "                                                          &#x27;invscaling&#x27;,\n",
       "                                                          &#x27;adaptive&#x27;],\n",
       "                                        &#x27;loss&#x27;: [&#x27;hinge&#x27;, &#x27;log_loss&#x27;,\n",
       "                                                 &#x27;modified_huber&#x27;],\n",
       "                                        &#x27;max_iter&#x27;: [2000, 2500, 3000, 4000,\n",
       "                                                     5000],\n",
       "                                        &#x27;penalty&#x27;: [&#x27;l2&#x27;, &#x27;l1&#x27;, &#x27;elasticnet&#x27;]},\n",
       "                   random_state=42,\n",
       "                   scoring=make_scorer(f1_score, response_method=&#x27;predict&#x27;, average=macro),\n",
       "                   verbose=2)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: SGDClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(random_state=42)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;SGDClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.SGDClassifier.html\">?<span>Documentation for SGDClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>SGDClassifier(random_state=42)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=SGDClassifier(random_state=42), n_iter=50,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'alpha': [1e-05, 0.0001, 0.001, 0.01],\n",
       "                                        'eta0': [0.01, 0.1, 0.5, 1],\n",
       "                                        'learning_rate': ['optimal',\n",
       "                                                          'invscaling',\n",
       "                                                          'adaptive'],\n",
       "                                        'loss': ['hinge', 'log_loss',\n",
       "                                                 'modified_huber'],\n",
       "                                        'max_iter': [2000, 2500, 3000, 4000,\n",
       "                                                     5000],\n",
       "                                        'penalty': ['l2', 'l1', 'elasticnet']},\n",
       "                   random_state=42,\n",
       "                   scoring=make_scorer(f1_score, response_method='predict', average=macro),\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Definiamo il modello base\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "\n",
    "# Definiamo la distribuzione degli iperparametri\n",
    "param_dist = {\n",
    "    \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\"],\n",
    "    \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "    \"max_iter\": [2000, 2500, 3000, 4000, 5000],\n",
    "    \"learning_rate\": [\"optimal\", \"invscaling\", \"adaptive\"],\n",
    "    \"eta0\": [0.01, 0.1, 0.5, 1],\n",
    "}\n",
    "\n",
    "# Scorer: f1 macro per bilanciare classi\n",
    "scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Setup RandomizedSearch\n",
    "random_search = RandomizedSearchCV(\n",
    "    sgd,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # puoi aumentare se hai tempo\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce91dea2-7991-4a0a-a139-db2b72c0ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      "{'penalty': 'l2', 'max_iter': 3000, 'loss': 'hinge', 'learning_rate': 'adaptive', 'eta0': 0.01, 'alpha': 1e-05}\n",
      "\n",
      "Validation set evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.89      0.89      7500\n",
      "           1       0.89      0.89      0.89      7500\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Valutazione su validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "print(\"\\nValidation set evaluation:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd3a1328-0715-496b-ab46-753066bde52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluation on test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      7500\n",
      "           1       0.89      0.90      0.90      7500\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "print(\"\\nFinal evaluation on test set:\")\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94ce0b22-6b99-485b-bd84-9050d7942f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88      7500\n",
      "           1       0.89      0.86      0.87      7500\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "print(\"\\nNaive Bayes:\")\n",
    "print(classification_report(y_val, nb.predict(X_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ad72619-5411-4935-a2a9-ba7c420b7793",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.54 TiB for an array with shape (70000, 10878793) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      3\u001b[0m lgb_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m lgb_model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y_train)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLightGBM:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_val, lgb_model\u001b[38;5;241m.\u001b[39mpredict(X_val\u001b[38;5;241m.\u001b[39mtoarray())))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ML-GPU\\lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.54 TiB for an array with shape (70000, 10878793) and data type float64"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=300, random_state=42)\n",
    "lgb_model.fit(X_train.toarray(), y_train)\n",
    "\n",
    "print(\"\\nLightGBM:\")\n",
    "print(classification_report(y_val, lgb_model.predict(X_val.toarray())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce61f387-019f-4c6e-a639-0074ffafcabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 200, 100)          2000000   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 198, 8)            2400      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 198, 8)            32        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 198, 8)            0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                128       \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,002,641\n",
      "Trainable params: 2,002,593\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.5278 - accuracy: 0.7331 - val_loss: 0.4137 - val_accuracy: 0.8235\n",
      "Epoch 2/25\n",
      "547/547 [==============================] - 2s 5ms/step - loss: 0.3429 - accuracy: 0.8605 - val_loss: 0.3583 - val_accuracy: 0.8431\n",
      "Epoch 3/25\n",
      "547/547 [==============================] - 2s 5ms/step - loss: 0.2515 - accuracy: 0.9075 - val_loss: 0.3887 - val_accuracy: 0.8401\n",
      "Epoch 4/25\n",
      "547/547 [==============================] - 2s 5ms/step - loss: 0.1879 - accuracy: 0.9339 - val_loss: 0.4358 - val_accuracy: 0.8395\n",
      "Epoch 5/25\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.1408 - accuracy: 0.9517 - val_loss: 0.5149 - val_accuracy: 0.8402\n",
      "Epoch 6/25\n",
      "547/547 [==============================] - 3s 5ms/step - loss: 0.1095 - accuracy: 0.9648 - val_loss: 0.5735 - val_accuracy: 0.8367\n",
      "Epoch 7/25\n",
      "547/547 [==============================] - 2s 5ms/step - loss: 0.0862 - accuracy: 0.9727 - val_loss: 0.6318 - val_accuracy: 0.8335\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Parameters\n",
    "max_words = 20000\n",
    "max_len = 200\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "X_train_nn, X_temp_nn, y_train_nn, y_temp_nn = train_test_split(padded, df[\"label\"], test_size=0.30, stratify=df[\"label\"])\n",
    "X_val_nn, X_test_nn, y_val_nn, y_test_nn = train_test_split(X_temp_nn, y_temp_nn, test_size=0.50, stratify=y_temp_nn)\n",
    "\n",
    "l2_lambda = 0.001\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    Conv1D(filters=8, kernel_size=3, use_bias=False), # Often disable bias before BN\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'), # Activation after BN\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(16, use_bias=False), # Often disable bias before BN\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'), # Activation after BN\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# EarlyStopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    X_train_nn,\n",
    "    y_train_nn,\n",
    "    epochs=25,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_nn, y_val_nn),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b09d5a03-bfa3-4287-8460-bf6d4b85ee87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CNN classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84      7500\n",
      "           1       0.83      0.87      0.85      7500\n",
      "\n",
      "    accuracy                           0.84     15000\n",
      "   macro avg       0.85      0.84      0.84     15000\n",
      "weighted avg       0.85      0.84      0.84     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_cnn = (model.predict(X_test_nn) > 0.5).astype(\"int32\")\n",
    "print(\"\\nCNN classification report:\")\n",
    "print(classification_report(y_test_nn, y_pred_cnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af658f06-453c-4aca-910a-d634268300b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "547/547 [==============================] - 13s 21ms/step - loss: 0.3851 - accuracy: 0.8310 - val_loss: 0.3041 - val_accuracy: 0.8751\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 0.2503 - accuracy: 0.9065 - val_loss: 0.3119 - val_accuracy: 0.8761\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 0.2009 - accuracy: 0.9273 - val_loss: 0.3319 - val_accuracy: 0.8754\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 11s 20ms/step - loss: 0.1666 - accuracy: 0.9415 - val_loss: 0.3663 - val_accuracy: 0.8731\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00004: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model definition\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len),\n",
    "    Bidirectional(LSTM(32, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Training\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train_nn, y_train_nn,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_nn, y_val_nn),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "772d0757-0ef1-4356-8c11-88b8d88d0a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RNN LSTM classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87      7500\n",
      "           1       0.85      0.91      0.88      7500\n",
      "\n",
      "    accuracy                           0.87     15000\n",
      "   macro avg       0.88      0.87      0.87     15000\n",
      "weighted avg       0.88      0.87      0.87     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_nn = (rnn_model.predict(X_test_nn) > 0.5).astype(\"int32\")\n",
    "print(\"\\nRNN LSTM classification report:\")\n",
    "print(classification_report(y_test_nn, y_pred_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a4529e1-de3b-46a0-b6b2-ccd8a0d0c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_index = {}\n",
    "glove_path = \"glove.6B.100d.txt\"  # Cambia path se serve\n",
    "\n",
    "with open(glove_path, encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vector\n",
    "\n",
    "print(f\"Loaded {len(embedding_index)} word vectors from GloVe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b5bdfbd-fd9e-4d04-9c02-e27566644283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero parole da usare e tokenizer già esistente\n",
    "word_index = tokenizer.word_index\n",
    "num_tokens = min(max_words, len(word_index)) + 1  # +1 per OOV\n",
    "\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a77306c9-525c-42cc-b4a7-279df74fe685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 200, 100)          2000100   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 200, 128)          64128     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 200, 128)          512       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 200, 128)          82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,204,645\n",
      "Trainable params: 2,204,389\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "547/547 [==============================] - 8s 12ms/step - loss: 0.4155 - accuracy: 0.8057 - val_loss: 0.3157 - val_accuracy: 0.8638\n",
      "Epoch 2/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.2642 - accuracy: 0.8942 - val_loss: 0.3254 - val_accuracy: 0.8593\n",
      "Epoch 3/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.1879 - accuracy: 0.9290 - val_loss: 0.3274 - val_accuracy: 0.8695\n",
      "Epoch 4/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.1290 - accuracy: 0.9535 - val_loss: 0.3431 - val_accuracy: 0.8745\n",
      "Epoch 5/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0847 - accuracy: 0.9711 - val_loss: 0.4334 - val_accuracy: 0.8624\n",
      "Epoch 6/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0592 - accuracy: 0.9809 - val_loss: 0.4564 - val_accuracy: 0.8564\n",
      "Epoch 7/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0442 - accuracy: 0.9859 - val_loss: 0.5084 - val_accuracy: 0.8687\n",
      "Epoch 8/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0368 - accuracy: 0.9881 - val_loss: 0.5383 - val_accuracy: 0.8697\n",
      "Epoch 9/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0290 - accuracy: 0.9911 - val_loss: 0.5611 - val_accuracy: 0.8661\n",
      "Epoch 10/50\n",
      "547/547 [==============================] - 6s 12ms/step - loss: 0.0245 - accuracy: 0.9925 - val_loss: 0.6825 - val_accuracy: 0.8595\n",
      "Epoch 11/50\n",
      "547/547 [==============================] - 6s 11ms/step - loss: 0.0225 - accuracy: 0.9934 - val_loss: 0.6236 - val_accuracy: 0.8629\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, LSTM, BatchNormalization, Reshape\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parameters\n",
    "max_words = 30000\n",
    "max_len = 200\n",
    "embedding_dim = 100\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"cleaned_review\"])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df[\"cleaned_review\"])\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Train/Val/Test split\n",
    "X_train_nn, X_temp_nn, y_train_nn, y_temp_nn = train_test_split(padded, df[\"label\"], test_size=0.30, stratify=df[\"label\"])\n",
    "X_val_nn, X_test_nn, y_val_nn, y_test_nn = train_test_split(X_temp_nn, y_temp_nn, test_size=0.50, stratify=y_temp_nn)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(input_dim=num_tokens,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=True),  # Fine-tuning gli embeddings\n",
    "\n",
    "    # Convolutional layers (2 stacked layers)\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    BatchNormalization(),  # Normalizza gli output\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu', padding='same'),\n",
    "    GlobalMaxPooling1D(),  # Poolea la feature map per ottenere il massimo da ciascuna feature\n",
    "\n",
    "    # Reshape output per passare al layer LSTM\n",
    "    Reshape((1, 128)),  # Aggiungi una dimensione per i timesteps, che è necessaria per l'LSTM\n",
    "\n",
    "    # LSTM layer per catturare le dipendenze temporali\n",
    "    LSTM(64, return_sequences=False),\n",
    "\n",
    "    # Fully connected layer\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Regularizzazione\n",
    "\n",
    "    # Output layer\n",
    "    Dense(1, activation='sigmoid')  # Output binario per la classificazione\n",
    "])\n",
    "\n",
    "# Compilazione del modello\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Training del modello\n",
    "history = model.fit(X_train_nn, y_train_nn,\n",
    "                    epochs=50,\n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_val_nn, y_val_nn),\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8f6bd6b-c1c5-4cb2-af97-0592acace2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "547/547 [==============================] - 16s 27ms/step - loss: 0.4152 - accuracy: 0.8116 - val_loss: 0.3270 - val_accuracy: 0.8659\n",
      "Epoch 2/10\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.2678 - accuracy: 0.8936 - val_loss: 0.2942 - val_accuracy: 0.8790\n",
      "Epoch 3/10\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.2145 - accuracy: 0.9168 - val_loss: 0.3028 - val_accuracy: 0.8789\n",
      "Epoch 4/10\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.1779 - accuracy: 0.9344 - val_loss: 0.3586 - val_accuracy: 0.8731\n",
      "Epoch 5/10\n",
      "547/547 [==============================] - 14s 26ms/step - loss: 0.1485 - accuracy: 0.9463 - val_loss: 0.3478 - val_accuracy: 0.8691\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Model definition\n",
    "rnn_model = Sequential([\n",
    "    Embedding(input_dim=num_tokens,\n",
    "          output_dim=embedding_dim,\n",
    "          weights=[embedding_matrix],\n",
    "          input_length=max_len,\n",
    "          trainable=True),    \n",
    "    Bidirectional(LSTM(64, return_sequences=False)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# Training\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train_nn, y_train_nn,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_nn, y_val_nn),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc7d5a-5a11-4e6c-bdb4-1634d2dbbd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
